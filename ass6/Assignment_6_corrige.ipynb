{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 Ex. 1 \n",
    "<b>Gradient Descendent Algorithm </b>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to find numerically the local minimum of the function $f(r_1, r_2)$, which lies near the point $ r_1= 1, r_2= 0$\n",
    "$$ f(\\mathbf{r})= \\sqrt{r_1^2+r_2^2} +sin(r_1)+2cos(r_2+0.5)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 6.1.1\n",
    "At first define two functions that compute $f$ and its gradient $\\nabla f$ (can be obtained through analytical differentiation)\n",
    "$$ \\nabla f =\\left[ \\frac{r_1}{\\sqrt{r_1^2+r_2^2} }+cos(r_1), \\frac{r_2}{\\sqrt{r_1^2+r_2^2}} -2sin(r_2+0.5)\\right]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3) #reduces the number of printed digits\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2\n",
    "Check for the point (1,0) if the gradient produce results consistent with the derivative obtained through numerical differentiation (via finite difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3\n",
    "Perfom the minimization of $f(\\mathbf{r})$ using a self-implemented version of the gradient descendent algorithm explained in the theory lesson:\n",
    "$$\\mathbf{r}_{k+1}=\\mathbf{r}_{k}-\\delta \\nabla f({\\mathbf{r}_{k}})\n",
    "$$\n",
    "\n",
    "For simplicity it is possible to assume $\\delta$ to be constant all throught the optimization. \n",
    "Try different $\\delta$ to find the optimal. What happens if $\\delta$ is too large or too small? (explain with a brief statment) \n",
    "\n",
    "You should iterate until a convergence criteria is fulfilled (eg $norm(\\nabla f(\\mathbf{r}_k))$<threshold).\n",
    "Also is adviseable to put a maximum limit to the iterations (say 1000 or more), just in case convergence it is not achieved.\n",
    "\n",
    "Perform the minimization twice, once starting from $\\mathbf{r}_0=(1,0) $ , and the second time from $\\mathbf{r}_0=(-1,-1) $. \n",
    "What happens in the two cases?\n",
    "\n",
    "HINTS: One idea could be to create a <b> while </b> loop or a <b> for </b> loop and iterate untill the convergence is achieved or untill the number of iterations excede the limit, than break the loop with the <b>break</b> statment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.4\n",
    "Compare your results with the ones obtained from the scipy minimization algorithm (scipy.optimize.minimize ) https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize as sp_minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.5 Visualizing the minimization  in 3D (Bonus)\n",
    "Create a meshgrid for a reasonable value interval of $r_1$ and $r_2$ and calculate the value of $f$ on the grid points:\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html\n",
    "\n",
    "Find the minimum value of $f$ on this grid and its corresponding values of $r_1$ and $r_2$\n",
    "\n",
    "plot the data using ax.plot_surface\n",
    "as explained:\n",
    "https://matplotlib.org/stable/gallery/mplot3d/contour3d_3.html,\n",
    "https://matplotlib.org/stable/gallery/mplot3d/subplot3d.html,\n",
    "\n",
    "Redo the two optimizations, this time saving in 3 separate arrays the values of $r_1,r_2,f$  for every points in the two optimization.\n",
    "\n",
    "Visualize the optimization process scattering or plotting these points in the same 3d axes as the surface\n",
    "\n",
    "Using the jupyter magic command \"%matplotlib notebook\" you can interact with the plot.\n",
    "To restore the fixed visualization the magic comand is \"%matplotlib inline\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#for interactive visualization\n",
    "ax = plt.figure().add_subplot(projection='3d')\n",
    "#ax.scatter()\n",
    "#ax.plot()\n",
    "#ax.plot_surface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([0,3],[0,1])  #the line is plotted on the 3d axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "#restore the matplotlib visualization\n",
    "plt.plot([0,1],[0,1])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 6.2  \n",
    "$\\textbf{Linear regression}$ In this exercise, you are going to familiarize yourself with the gradient descent algorithm, by performing linear regression on different datasets from Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.1\n",
    "Load the dataset ex6_2.dat. It contains two columns of $x$- and $y$-values respectively. Plot the data in a scatter plot.\n",
    "\n",
    "The observed data can be modelled through a regression line: \n",
    "$$y^{L.R.} = f(x)= wx+b$$ <br>\n",
    "From your intuition give a reasonable guess of the slope $w_0$ and of the intercept $b_0$.\n",
    "Add the initial guessed regression line to the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.2\n",
    "In order to find a proper set of regression line for the data, a common approach is to choose the cofficients ($w,b$), such that they minimize the p-norm of the error vector ( $y_k-f(x_k;w,b)$ ) evaluated by the loss function $ E^p(w,b)$\n",
    "\n",
    "\n",
    "$$ E^p(w,b)=\\left(\\frac{1}{N}\\sum\\limits_k\\left|(y_k-f(x_k;w,b)\\right|^p\\right)^\\frac{1}{p}$$\n",
    "\n",
    "Using the $x,y$ data read previously, implement the function $E^p(w,b)$ for a generic $p$ (defining a function E([w,b],p) ).\n",
    "Than calculate the mean absolute error $E^1(w_0,b_0)$, the least square error $E^2(w_0,b_0)$, but also $E^{10}$,$E^{100}$ and $E^{200}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.3\n",
    "An other loss function is the maximum error:\n",
    "$$ E^\\infty = max|y_k-f( x_k)|\n",
    "$$\n",
    "Create a function to compute $E^\\infty$ and compare it to the previous loss functions, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.4\n",
    "Optimize the parameter w and b for the loss functions $E^1,E^2,E^3,E^{5},E^{10},E^{\\infty}$ .\n",
    "HINT You can use the scipy.optimize.minimize function, doing so you should join the variables to be optimized (w and b) in an array, and the fixed value of the parameter p should be specified in the args() . \n",
    "Save the optimized w and b parameters (e.g. in a dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2.5\n",
    "Scatter again your data, drawing the regression lines obtained minimizing  $E^1,E^2,E^{10},E^{100},E^{\\infty}$, add a legend to the plot\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 6.3\n",
    "\n",
    "$\\textbf{Polynomial regression - basis functions.}$  In this exercise you are going to fit non-linear data to an expansion over a set of basis functions $\\phi_b(x)$ \n",
    "$$ f(x;\\mathbf{w}) = \\sum_b^{N_{BF}} w_b \\phi_b(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.1\n",
    "$\\texttt{ex6_3.dat}$ contains another dataset of $x$- and $y$-values, load the file and plot the data. <br>\n",
    "The curve represents the energy (in atomic units) as a function of the distance between two atoms (in Angstrom: $1 A=10^{-10}m)$. <br>\n",
    "You will find that this time, there is no linear dependece between $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.2\n",
    "As a first attempt, use polynomials up to degree 5 as basis functions ($\\phi_b=x^b ; b \\in [0,5]$). <br>\n",
    "First, build the Vandermonde-Matrix ($\\textbf{Hint}$: You can use $\\texttt{numpy.vstack}$ to create the Vandermonde-Matrix): \n",
    "\\begin{equation}\n",
    "    V_{kb}=x_k^{b}  \\ \\ \\ ( b \\in [0,5])\n",
    "\\end{equation}\n",
    "\n",
    "It is also possible to use the $\\texttt{numpy.vander}$ function.\n",
    "https://numpy.org/doc/stable/reference/generated/numpy.vander.html <br>\n",
    "The Vandermond matrix is useful because allow a quick extimation of the value of the fitting polynomial at the $x_k$ points for different values of $\\mathbf{w}$\n",
    "\n",
    "$$ f(x_k;\\mathbf{w}) = \\sum_{b=0}^{N_{BF}} x_k^b  w_b = \\sum_{b=0}^{N_{BF}} V_{kb} w_b$$\n",
    "\n",
    "For the initial guess $\\mathbf{w}_0=[8, -18.5,  18.,  -9,   2.2,  -0.2]$, plot the polynomial together with the curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0=np.array([8, -18.5,  18.,  -9,   2.2,  -0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.3\n",
    "To find the optimal values of $\\mathbf{w}$ minimize the fitting using the least squares method. <br>\n",
    "Create a loss function $E^2$ for this problem.\n",
    "$$ E^2(\\mathbf{w})=\\left(\\frac{1}{N_{points}}\\sum\\limits_k\\left|(y_k-f(x_k;\\mathbf{w})\\right|^2\\right)^\\frac{1}{2}$$\n",
    "\n",
    "Minimize $E^2$ using scipy. Compare the results with the ones calculated from the numpy.polyfit function.\n",
    "Be aware on which order are the weights outputted by numpy.polyfit https://numpy.org/doc/stable/reference/generated/numpy.polyfit.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.4\n",
    "Now if:\n",
    "$$ \\vec{y}= V \\vec{w} + \\vec{\\epsilon}\n",
    "$$\n",
    "\n",
    "multiplying the left and the right hand side by the pseudo inverse matrix of $V$ (https://inst.eecs.berkeley.edu/~ee127/sp21/livebook/def_pseudo_inv.html)\n",
    "$$ V_{pseudoinv}= (V^TV)^{-1}V^T $$\n",
    "\n",
    "\n",
    "$$  (V^TV)^{-1}V^T \\vec{y}=   (V^TV)^{-1}V^T (V \\vec{w} + \\vec{\\epsilon}) $$\n",
    "The equation is projected on the linear algebra subspace defined by the columns of $V$.\n",
    "and choosing the coefficients vector $w$ equal to:\n",
    "$$  \\vec{w}=(V^TV)^{-1}V^T \\vec{y} $$\n",
    "Produce an error vector $\\epsilon$ which is orthogonal to the subspace defined by the columns of V:\n",
    "$$(V^TV)^{-1}V^T \\vec{\\epsilon} = 0 $$\n",
    "$$V^T \\vec{\\epsilon} = 0 $$\n",
    "\n",
    "This is indeed the least square solution to the fitting. <br>\n",
    "Verify that the $w$ calculated in this way coincides with the output of the numpy.polyfit function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3.5 (BONUS) \n",
    "It seems polynomials are not ideal basis functions to fit this data. <br>\n",
    "The Lennard-Jones potential $U_{LJ}(\\vec{r}_{ab})$ can be used to compute pairwise interactions between atoms $a$ and $b$.\n",
    "$$\n",
    "U_{LJ}(\\vec{r}_{ab}) = 4\\epsilon \\left(\\left[\\frac{\\sigma}{r_{ab}}\\right]^{12} - \\left[\\frac{\\sigma}{r_{ab}}\\right]^6 \\right)\n",
    "$$\n",
    "\n",
    "Optimize $\\epsilon, \\sigma$ using the least squares fitting. <br>\n",
    "Than plot again your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps,sigma=0.1,1\n",
    "def U_lj(r,eps=eps,sigma=sigma):\n",
    "    return 4*eps*((sigma/r)**12-(sigma/r)**6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
