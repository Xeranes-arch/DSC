{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nsa5dmzQbdgv"
   },
   "source": [
    "# Assignment 9: \n",
    "<br>\n",
    "\n",
    "<li>Unsupervised Learning using K-Means \n",
    "    <li>Cross-Validation of the Kernel Ridge Regression (supervised model)\n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "Unsupervised machine learning is used to analyse and cluster unlabeled datasets. They are called unsupervised since they do not require additional human intervention and, thereby, learn underlying patterns, clusterings, similarities or differences themselves. In contrast, in supervised learning algorithms the label is known and through the use of clever optimization, a machine learning model is trained to reproduce the given labels as good as possible. Unsupervised machine learning algorithms are most commonly used for exploratory data analysis (EDA), recommendation systems (\"customers who bought XYZ also bought items .....\") and also image recognition and generation.\n",
    "\n",
    "\n",
    "For this assignment, we will rely on the scikit-learn (or sklearn) Python library, which is one of the most commonly used libraries for classical machine learning. Sklearn contains various algorithms useful for classification, regression, clustering, dimensionality reduction, model selection and preprocessing. It is worth having a look at the webpage to familiarize yourself with possible tools that could be useful for any future ventures https://scikit-learn.org/stable/index.html. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BMk3dLSYL7A"
   },
   "source": [
    "## 9.1 $\\textbf{Unsupervised Learning: the $\\texttt{K-Means}$ algorithm }$ \n",
    "\n",
    "A common unsupervised learning algorithm for clustering is $\\texttt{K-Means}$. The goal of clustering is to separate datapoints into different classes. In the most simple case this can just be points in an x-y plane. In practical applications clustering can be useful for analysing images of stars, crystals or biological cells.\n",
    "\n",
    "$\\texttt{K-Means}$ works in the following way:\n",
    "1. Initilization: randomly choose $k$ centroids (means).\n",
    "2. Assignment: assign each sample in your dataset to the cluster with closest centroid.\n",
    "3. Update: Calculate the new centroid (mean) of each cluster.\n",
    "\n",
    "This algorithm is performed for a certain number of steps and will converge (cluster assignment will not change anymore) at a certain point. \n",
    "\n",
    "For a detailed description see e.g., [Wikipedia](https://en.wikipedia.org/wiki/K-means_clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIcoX0dRYL7F"
   },
   "source": [
    "### 1.1 \n",
    "Load the dataset $\\texttt{ex_kmeans.txt}$ from Moodle. It contains 2 columns corresponding to $x$ and $y$ coordinates of each sample. Visualize the dataset in a scatter plot and answer how many clusters you can recognize in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjG7D_bNYL7J"
   },
   "source": [
    "### 1.2\n",
    "Implement as a costum function your own version of the $\\texttt{K-Means}$-algorithm. \n",
    "\n",
    "At first randomly choose $k$ of your datapoints as initial centroids. \n",
    "\n",
    "Then, compute the euclidian-distance (L2-Norm) from each point to every centroid and assign each datapoint to the cluster with closest centroid.\n",
    "\n",
    "With your updated clusters you can recompute the centroid of each cluster, by taking the mean of each assigned sample (these new centroinds are no longer datapoints). \n",
    "\n",
    "Iteratively update each cluster for a large number of iterations (e.g., 100 times), break the loop if convergence is reached ( no difference between old and the new centroids). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCEnAPecYL7K"
   },
   "source": [
    "### 1.3\n",
    "Choosing an appropriate number k of clusters, perform a k-means clustering of the dataset and plot the results, showing each cluster in a seperate color.\n",
    "\n",
    "If you did not manage to solve ex 9.1.2, use the KMeans implementation of sklearn instead via `from sklearn.cluster import KMeans`. You can use the $\\texttt{.fit()}$ method on your KMeans object to perform the clustering, and access the assigned cluster labels of each sample in the $\\texttt{.labels_}$ attribute of the object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgtP-e0vYL7M"
   },
   "source": [
    "### 1.4 \n",
    "Run the clustering a few times choosing each time a different random seed/state. Does your algorithm always converge to the \"correct\" clustering? What could be a reason if it does not converge \"correctly\", e.g. assigning the top left as two respective clusters? Give an explanation on how a different initialization method (e.g., $\\texttt{K-Means++}$ https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html ) solves the problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nOnVisxYL7N"
   },
   "source": [
    "## 9.2 $\\textbf{Clustering of Lattice Keypoints.}$ \n",
    "\n",
    "In this exercise we are going to apply clustering to a set of descriptors of keypoints in a microscopy image. The keypoints and descriptors can be generate via a computer-vision algorithm called SIFT, that detects keypoints and stores local image information in a descriptor. These descriptors can be further analyzed in an unsupervised learning setting to seperate different sublattices in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L40sfE3SYL7O"
   },
   "source": [
    "### 2.1\n",
    "Load the data from \" microscopy_img.pkl \" found on Moodle, using $\\texttt{pickle}$. The file contains a dictionary with an image, keypoints, and one descriptor for each keypoint that describes some local image information. Each keypoint contains $x$- and $y$-coordinates of a lattice site in the image. The corresponding descriptors will later be used in a clustering algorithm to seperate features of your microscopy image into different clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GznOQN-iYL7P"
   },
   "source": [
    "### 2.2\n",
    "Make a plot of two subplots to visualize the image and the image with the positions and size of the detected keypoints on top, respectively. Consider using $\\texttt{plt.imshow}$ and $\\texttt{plt.scatter}$. \n",
    "\n",
    "$\\textbf{Hint:}$ The keypoints array contains three columns, corresponding to $\\texttt{x}$-, $\\texttt{y}$-positions and the size of the detected keypoint. You can also rescale the size, for better visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7eaHnnyYL7R"
   },
   "source": [
    "### 2.3 \n",
    "Perform a clustering of the descriptors using $\\texttt{sklearn.cluster.KMeans}$ (it provides better initialization methods than our custom implementation) by initializing $k=2$ and visualize the result by plotting each cluster in a seperate color in the image. By setting $k=2$, light and dark keypoints should be nicely seperated. \n",
    "\n",
    "$\\textbf{Hint}:$ You can use the $\\texttt{.fit()}$ method on your clustering object to perform the clustering, and access the assigned cluster labels of each sample in the $\\texttt{.labels_}$ attribute of the object.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG-RHYKJYL7T"
   },
   "source": [
    "### 2.4 \n",
    "Try to further optimize your clustering by varying your number of clusters $k$ in the $\\texttt{K-Means}$-algorithm and visualize the predicted clusters in your image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyXwbzLJYL7T"
   },
   "source": [
    "### 2.5\n",
    "Give an explanation on why you would choose a certain $k$ as optimal. What quantitative information can you extract by seperating your keypoints in clusters? What information could you extract by looking only at the cluster containing the largest amount of samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ixEyIMegYL7c"
   },
   "source": [
    "## 9.3 $\\textbf{Cross Validation }$ \n",
    "In this exercise we are going to build a regression model to predict the bonding energy of two hydrogen atoms based on their bond length. Machine learning to accurately predict quantum chemical properties of molecules and materials is an active field of research. Conventionally, such quantum properties are calculated using methods to approximate solutions to Schr√∂dingers equation, which depending on the level of theory take hours, to days, to months to compute. Supervised machine learning algorithms in that regard hold the promise to massively accelerate this process yielding models with milliseconds of execution time, while being as accurate as quantum mechanical approximations that take magnitudes longer to compute.\n",
    "\n",
    "As a regression model, we are going to use the supervised machine learning algorithm Kernel-Ridge-Regression (KRR), which combines ridge regression (linear least squares with l2-norm regularization) with the so called 'kernel trick'. The word 'kernel' is used in mathematics to denote a weighting function for a weighted sum or integral.\n",
    "\n",
    "In ridge regression, a mapping function is learned, relating a representation vector $\\textbf{x}$ to a label $y$.\n",
    "The 'kernel trick' renders the problem linear through the use of kernel functions that yield the inner product of high dimensional representations $\\textbf{x}$.\n",
    "In practice, KRR is formulated as follows:\n",
    "\n",
    "$$    y(\\textbf{x}_{q}) = \\sum^N_i w_i \\;  K(\\textbf{x}_i, \\textbf{x}_{q}) $$\n",
    "\n",
    "with $w_i$ being the regression coefficient, $\\textbf{x}_i$ being the i-th representation vector of the training set, $\\textbf{x}_q$ being the query representation and kernel function $K$. \n",
    "The regression coefficients $w$ are calculated through a closed-form solution:\n",
    "\n",
    "$$ w = (\\textbf{K} + \\alpha \\textbf{I})^{-1} \\textbf{y}^{\\textrm{train}} $$\n",
    "\n",
    "with the identity matrix $\\textbf{I}$ and a regularization coefficient $\\alpha$. \n",
    "The purpose of the regularization coefficient is to prevent overly large regression weights and, thus, overfitting to the training data.\n",
    "The radial-basis function kernel is a popular choice and has the following form:\n",
    "\n",
    "$$      K(\\textbf{x}_i, \\textbf{x}_j) = \\exp\\left(-\\frac{||\\textbf{x}_i - \\textbf{x}_j||^2_2}{2\\sigma^2}\\right) $$\n",
    "\n",
    "with $\\gamma$ being the kernel width.\n",
    "\n",
    "This leaves us with two hyperparameters, the regularization strength $\\alpha$ and the kernel width $\\gamma$, which have to be determined through a process called hyperparameter optimization. Note, the notation $\\alpha$ and $\\gamma$ can be named differently in publications or Python libraries, in this exercise we stick to the sklearn convention. Our goal is to find the optimal values that yield the best possible performance. However, we do not want to overfit our model to our dataset and, thus, lose generality. \n",
    "\n",
    "To avoid this, we will use cross-validation and out-of-sample-testing. In general, cross-validation is a resampling method using different portions of the data to test and train a model on different iterations. You can read more about cross-validation procedures [here](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "First, the whole dataset is split into a training and test set. Common ratio's are 80/20 or 70/30 (in reality this obviously depends on your total dataset size and distributions). We use the training set to optimize the hyperparameters via cross-validation and then test the final model performance on the separate test-dataset. It is very important to be precise here, accidentally optimizing the hyperparameters on the test set or testing on the training set leads to massively overoptimistic results! It is important to always be sceptical when your results are 'too good to be true'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HCP_k9nYL7d"
   },
   "source": [
    "### 3.1\n",
    "The training and test split has already been prepared and can be found on Moodle in the files $\\texttt{h2_train.txt}$ and $\\texttt{h2_test.txt}$. Both files contain the energies an $H_2$ molecule as a function of the bond length. \n",
    "\n",
    "The testing data contains some randomly chosen points, on which you can perform a final test on your models generality. Load the data and plot both datasets into a single plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOVVQ-uQYL7f"
   },
   "source": [
    "### 3.2 \n",
    "To choose an optimal regression model predicting our data, we need to optimize it's hyperparameters. This can be done by performing k-fold cross-validation on our training data. In k-fold cross-validation, the dataset is split into k-folds, each containing some portion of the entire dataset. By training the model $k$ times on $k-1$ of these folds and validating it on the remaining fold, we can assess the mean score and it's standard deviation for the case of training and validation data. \n",
    "\n",
    "Use the [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) class of sklearn to split your data into 5-splits.\n",
    "Make a plot containing a 5x2 subplot grid to visualize the histograms of train and validation features and labels of each split, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cw5PnmiL1y2W"
   },
   "source": [
    "### 3.3\n",
    "\n",
    "Why can it be helpful to visualize these distributions, especially the label distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0nR9K1yg0Td"
   },
   "source": [
    "\n",
    "\n",
    "*   Helpful in low data scenarios, different splits can lead to large performance differences\n",
    "*   Helps to assess whether you are extrapolating. If your label is at the tail of the distribution it will be hard\n",
    "*   You could also consider to truncate your dataset based on the distribution, think about outliers from experimental measurements\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7M51JQwYL7f"
   },
   "source": [
    "### 3.4 \n",
    "\n",
    "Use $\\texttt{sklearn.kernel_ridge.KernelRidge}$ to fit a model and optimize the regularization-parameter ($\\texttt{alpha}$ in the regression object) and the kernel-parameter ($\\texttt{gamma}$) by performing cross validation. Use an radial basis function kernel, which takes following form\n",
    "\n",
    "$$ K(\\textbf{x}_i, \\textbf{x}_j) = \\exp\\left(-\\gamma||\\textbf{x}_i - \\textbf{x}_j||^2\\right) $$\n",
    "by choosing $\\texttt{kernel='rbf'}$. \n",
    "\n",
    "\n",
    "First, fix $\\texttt{gamma}$ and vary $\\texttt{alpha}$ over multiple orders of magnitude. To asses the fitting quality, use the $\\texttt{KernelRidge.score(x,y)}$ method ($\\textbf{Hint:}$ a score of 1 corresponds to a perfect fit and the score can get arbitrarily worse; see $R^2$-score). After that, fix $\\texttt{alpha}$ to an optimal value and determine the optimal $\\texttt{gamma}$ by repeating the procedure.\n",
    "\n",
    "Plot your results in two subplots showing the mean R2 score and it's standard deviation (errorbars) as a function of $\\texttt{alpha}$ and $\\texttt{gamma}$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zzfs0PjvYL9y"
   },
   "source": [
    "### 3.5 \n",
    "Use the optimal values for $\\texttt{alpha}$ and $\\texttt{gamma}$ to retrain your regressor on the full training dataset and predict the test data set to check if your method worked and provides a good fit. \n",
    "\n",
    "For this create a plot of two subplots showing the true values against the predicted values in a scatter plot, for the training and the test data respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SeyhaxmeDNde"
   },
   "source": [
    "### 3.6\n",
    "Instead of doing a grid based hyperparameter search we could have done a random search, varying both variables randomly for n-times. What are some advantages or disadvantages of grid vs. random search? What other approaches you could think of?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Assignment_9_solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
