{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ex 10.1\n",
    "\n",
    "An introduction to Pytorch\n",
    "A more tutorials on how to run pytorch can be found at: https://pytorch.org/tutorials/ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn,tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 PyTorch Tensors\n",
    "https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html\n",
    "\n",
    "The basic data type in PyTorch are tensors, they resemble Numpy Arrays, but are optimized to be used inside the Machine Learning Model. <br> \n",
    "In order to familiarize with tensors: <br>\n",
    "1)Construct one tensor from a Python list <br>\n",
    "2)Construct one tensor from a random numpy array <br>\n",
    "3)Generate three tensors (one with random elements, one with all elemnts equal to 1, the third with all zeros) of shape (3,3) using the relative functions inside the module torch. <br>\n",
    "4)Notice that tensors, like numpy arrays, support most of the standard mathematical operations, compute three different matematical operations (based on your choice) using the previously generated tensors <br>\n",
    "5)Transform the results of your operations to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 DataSets\n",
    "In Pytorch data are grouped in Datasets, to define a costum dataset, you need to derive a costum class from the torch.utils.data.Dataset class (see class inheritance in Python).<br>\n",
    "A Dataset class must implement three functions: __init__, __len__, and __getitem__ , as in the following example:\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files\n",
    "\n",
    "\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    from torchvision.io import read_image\n",
    "    from torch.utils.data import Dataset\n",
    "    \n",
    "    class CustomImageDataset(Dataset):\n",
    "        def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "            self.img_labels = pd.read_csv(annotations_file)\n",
    "            self.img_dir = img_dir\n",
    "            self.transform = transform\n",
    "            self.target_transform = target_transform\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.img_labels)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "            image = read_image(img_path)\n",
    "            label = self.img_labels.iloc[idx, 1]\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            if self.target_transform:\n",
    "                label = self.target_transform(label)\n",
    "            return image, label\n",
    "In the example above the initializer saves a list of file names (img_labels) in the Dataset instance.\n",
    "The \\__getitem__(n) function reads from the hard drive the nth. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)Create two random tensors (X,Y) of length 200.<br>\n",
    "2)Define a costum dataset which contains as objects two tensors (X and Y). The method \\__getitem__(n) should return the nth values of these tensors\\\n",
    "3) Test if the code works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Neural Layers\n",
    "Neural layers are function which apply transformations to tensors.\n",
    "The linear transformation ($ y=x A^T +b$) is among the simplest:\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
    "\n",
    "1) create a linear layer which has both as input and output one element tensors. <br>\n",
    "2) Find the parameters ($a$ and $b$) of the 1D transformation, trying to apply the transformation to different tensor (notice that the paramenter are randomly created every time you create a new layer)<br>\n",
    "3) concatenate two layers in order to produce from a scalar imput an inter 3 elements tensor and a 9 element tensor in the end, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Non linear activation functions\n",
    "In the pytorch docs: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "\n",
    "One way to introduce non linearity in machine learning models is through the use of activation functions. \n",
    "\n",
    "1) Import from pytorch the Rectified Linear Unit function \" ReLU \"  and the hyperbolic tangent \"tanh\" function\n",
    "2) Create a tensor of 100 evenly spaced points between -1 and 1\n",
    "3) Plot the outputs of the two activation functions taking as argument the previously defined vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import relu, tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Loss functions \n",
    "The way to measure the error of a Machine Learning prediction is through the use of one of the loss function. A list of the loss functions implemented in pytorch can be found here:\n",
    "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "\n",
    "One of the most important loss functions for regression problems is the Mean Square Error (MSE loss function)\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\n",
    "* Create two random tensors of length 4 \n",
    "* Compute the mean squared difference of the two vectors using the pytorch's MSE Loss function \n",
    "* Compute the mean squared difference of the two vectors using standard matematical operations on tensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Neural Networks for Classification\n",
    "Here we will provide a code snippet that classifies a dataset of handwritten digits. The layout of the code will provide you with insights and by adapting it, you will solve a classification problem in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will download the dataset and put it into datastructures provided by Pytorch. The transforms used here are specific to the dataset and transform the images into a `torch.tensor`, which is the datastructure used by pytorch (for an in depth tutorial on how Pytorch works and can be used for custom problems you can visit the [tutorials](https://pytorch.org/tutorials/)). In a second step the data points are normalized, which is a usual preprocessing step when training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we just create a directory \"mnist\" to store the dataset in\n",
    "!mkdir mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "dataset_train = datasets.MNIST('./mnist/', train=True, download=True,transform=transform)\n",
    "dataset_test = datasets.MNIST('./mnist/', train=False,transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=16)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can have a look at the dataset to get a feeling on how it looks. The goal of the neural network is to use the image as an input and predict the number the image shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2),dpi=100)\n",
    "for k,i in enumerate(np.random.randint(0, len(dataset_train),5)):\n",
    "    plt.subplot(1,5,k+1)\n",
    "    img, label = train_loader.dataset[i]\n",
    "    img = img[0].numpy()\n",
    "    print(label,img.shape)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Label = '+str(label))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this prediction we will use a simple feedforward neural network architecture with one hidden layer and  ReLU-activation function. This activation function has excellent properties for training neural networks, since they solve the vanishing gradient problem and allow for deeper neural networks, but there are many different activation functions that can be used (see the [documentation](https://pytorch.org/docs/stable/nn.html) for a full list).\n",
    "$$\\mathrm{ReLU}(x) = max(0,x)$$\n",
    "As output activation we use a logarithmic softmax, which can be seen as a mapping to the likelihoods of an image belonging to a certain class.\n",
    "\n",
    "The architecture of our neural network consists of two fully connected layers defined in `__init__(self)`, which first map the $28*28=784$ pixels to 128 intermidiate values via matrix multiplication and an added bias term. This intermidiate vector is then mapped to a 10-dimensional ouput vector. The `forward(self, x)` method defines the forward pass of the neural network, where first the image is flattened into a vector, then the first fully connected layer is applied, then a ReLU activation, then the second fully connected layer and finally the logarithmic softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining our architecture, we need to perform the training of the parameters. This optimization procedure is in most instances based on a gradient descent variety. Since we have a lot of training samples here, computing the full gradient of the loss function with respect to each adaptable weight is very expensive. Instead we opt for an approximation of the gradient of a minibatch of samples (e.g. 16 samples). This gives an approximate gradient direction, which allows us to update our weights and minimize our loss function more efficiently. Below you find a loop over our training and testing dataset. Read through the code and try to understand what each statement means and what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the loop below and train the model (this might take a while)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations\n",
    "epochs = 2\n",
    "# step width in gradient-based optimization\n",
    "learning_rate = 0.0001\n",
    "# initialize our model with random parameters\n",
    "model = Net()\n",
    "# choose an optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# choose a loss function\n",
    "loss_function = F.nll_loss\n",
    "\n",
    "# initialize lists for plotting\n",
    "learning_curve_train = []\n",
    "learning_curve_test = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # temporary variables to track model convergence\n",
    "    loss_train = []\n",
    "    loss_test = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Training Loop\n",
    "    for data, target in train_loader:\n",
    "        # set gradient to zero\n",
    "        optimizer.zero_grad()\n",
    "        # predict with current model\n",
    "        output = model(data)\n",
    "        # compute loss of predictions and target values\n",
    "        loss = loss_function(output, target)\n",
    "        # Backpropagation of errors gives gradients\n",
    "        loss.backward()\n",
    "        # Update weights with approximate gradient\n",
    "        optimizer.step()\n",
    "        # Store loss\n",
    "        loss_train.append(loss.item())\n",
    "\n",
    "    # Stop tracking gradients for evaluation phase\n",
    "    with torch.no_grad():\n",
    "        # Test Loop\n",
    "        for data, target in test_loader:\n",
    "            # predict with current model\n",
    "            output = model(data)\n",
    "            # compute loss\n",
    "            loss = loss_function(output, target)\n",
    "            # store loss\n",
    "            loss_test.append(loss.item())\n",
    "            # transform output into predicted class\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compare predictions and labels and store \n",
    "            # number of correct predictions\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Store loss at each iteration for plotting\n",
    "    learning_curve_train.append(np.mean(loss_train))\n",
    "    learning_curve_test.append(np.mean(loss_test))\n",
    "    \n",
    "    # Print to Stdout\n",
    "    print('Epoch: {}\\tTrain loss: {:.4f}\\tTest loss: {:.4f}\\tTest Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        epoch, learning_curve_train[-1], learning_curve_test[-1], \n",
    "        correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check if the model generalizes by looking at the learning curve (loss of training and testing data at each iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, epochs+1), learning_curve_train, label='Train')\n",
    "plt.plot(np.arange(1, epochs+1), learning_curve_test, label='Test')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can look at some of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2),dpi=100)\n",
    "for k,i in enumerate(np.random.randint(0, len(dataset_test),5)):\n",
    "    plt.subplot(1,5,k+1)\n",
    "    img, label = test_loader.dataset[i]\n",
    "    pred = torch.argmax(model(img)).item()\n",
    "    img = img[0].numpy()\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title('Prediction = '+str(pred))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.xlabel('True = '+str(label))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mim,Mim=(test_loader.dataset[7][0].numpy()).min() ,(test_loader.dataset[7][0].numpy()).max() \n",
    "mim,Mim,Mim-mim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 10.2\n",
    "\n",
    "## Classification of Stars \n",
    "In this exercise we are going to adapt the previously provided code to work on a comparatively small dataset of star measurements from [Kaggle](https://www.kaggle.com/deepu1109/star-dataset). From just four measurements (Temperature, Luminosity, Radius, Absolute magnitude), we will try to predict the star type (Browm Dwarf, Red Dwarf, White Dwarf, Main Sequence, Supergiant, Hypergiant) by using a Feedforward Neural Network architecture that was trained on 200 measurements. By determining hyperparameters we will optimize our model before we will test the generalization of the model to not seen instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1\n",
    "First load the dataset in `star_data.csv` from Moodle with pandas and familiarize yourself with the dataset. Extract the data into Numpy-arrays called `x` and `y` for input (the four measurments) and output (the star type) data respectively and make sure that the each array has the correct datatype (`np.float32` for `x` and `np.longlong` for `y`. **Hint**: you can use `.to_numpy()` on pandas dataframes and `.astype()` on numpy arrays to change the datatype."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2\n",
    "Write a function `rescale(x)` that min-max-scales each of your input-values to values between 0 and 1 (see [here](https://towardsdatascience.com/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79) for an introduction). \n",
    "$$x_{i,rescaled}=\\frac{x_i-x_{i,min}}{x_{i,max}-x_{i,min}}$$\n",
    "\n",
    "Split the pairs of inputs and outputs into 200 training samples and 40 testing samples. Shuffle your data beforehand to prevent unexpected behaviour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3\n",
    "In the introductory example on the MNIST dataset, the dataset was already provided in a clean datastructure which allowed for convenient usage. Now we have to create our own dataset. For this we will use the `Dataset` class provided in Pytorch. Complete the `__len__(self)` and `__getitem__(self, idx)` methods to return the number of samples in the dataset and the sample (input and label) with index `idx`. Print the number of training- and test-datasets and output some of the input and output pairs by calling 5 random indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `DataLoader` class, create a dataloader of your training- and test-data. You can specify a batch size for loading the data. This batch size can be seen as a hyperparameter, which allows you to tune the convergence of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4\n",
    "Adapt the previously provided `Net` class to work with the new data. For this change the architecture to reflect the new input and output dimensions of your data. You can also add additional layers and change the sizes of the hidden layers to tune model performance. **Hint:** Remove the `flatten()` function, since our data is already a vector and not a matrix. You can change the width of your hidden layers by changing the `out_features`-parameter in a `nn.Linear`-layer and the `in_features` of the following `nn.Linear`-layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5\n",
    "Use the provide code of the training and evaluation loop to the train your new model on the star data. Tune the hyperparameters (i.e., number of hidden layers, width of hidden layers, used activation functions, learning rate, number of epochs, training batch size) to optimize the training procedure and model performance and describe the influence of the different hyperparameters on the model performance and the training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6\n",
    "Visualize the learning curves of training and test data by plotting the loss at each epoch against the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7\n",
    "With your final model, predict the classes of the training and testing set one last time and visualize the corresponding confusion matrices. To do so, remember that the output of your model consists of a likelihood\n",
    "for each class. Use the `argmax()` function to get the predicted class\n",
    "label. To transform your torch tensors to python data you can use the\n",
    "`.item()`-method. You can then use `sklearn.metrics.confusion_matrix()` (https://en.wikipedia.org/wiki/Confusion_matrix) to obtain\n",
    "the confusion matrices, and `plt.imshow` to visualize them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
